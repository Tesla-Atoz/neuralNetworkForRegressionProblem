# -*- coding: utf-8 -*-
"""Artificial Neural Network for regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AEZGUu0s_AZKlaCG0Ph-_KJEc0461-gD

# Artificial Neural Network

### Importing the libraries
"""

import numpy as np
import pandas as pd
import tensorflow as tf

tf.__version__

"""## Part 1 - Data Preprocessing

### Importing the dataset
"""

# reading the dataset excel format.
dataset = pd.read_excel('Folds5x2_pp.xlsx')
# Extracting all the features 
X = dataset.iloc[:, :-1].values
# Extracting the target variable
y = dataset.iloc[:, -1].values

#taking a look at features from the dataset
print(X)

#taking a look at target variable
print(y)

"""### Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
#Splitting the dataset into train and test with  4:1
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

"""## Part 2 - Building the ANN

### Initializing the ANN
"""

ann = tf.keras.models.Sequential()

"""We can add any number of layers with any number of neurons or nodes. There is no fixed-procedure to get the values of them. It is the job of practitioner to experiment and get the best model. 

**Note:** Keras is included in Tensorflow 2.0 and higher versions.

**PS** : *Neurons individually cannot do much but layers of them combined together possess super-power. Just like an ant cannot do much but colony of them can build an anthill.*

### Adding the input layer and the first hidden layer
"""

# the first hidden layer with 6 neurons or nodes with rectifier activation function. 'relu' is the code for it.
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

"""### Adding the second hidden layer"""

# adding another hidden layer with 6 neurons or nodes with rectifier activation function.
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

"""### Adding the output layer"""

#finally, adding the output layer with only one node.
#Note here , I didnt provided activation kwarg coz for regression we donot need to 
#provide activation at the output coz output is continuous.(Basically thats what regression means)
ann.add(tf.keras.layers.Dense(units=1))

"""## Part 3 - Training the ANN

### Compiling the ANN
"""

#compiling the neural network means configuring it with some defaults
#to get optimal weights, optimization is necessary, I used 'adam' optimiser here.
ann.compile(optimizer = 'adam', loss = 'mean_squared_error')

"""### Training the ANN model on the Training set"""

#this part is favourite. Fitting the model with training set.
# i did batch learing with batch_size =32, and epochs =100

# we can play with those kwargs
ann.fit(X_train, y_train, batch_size = 32, epochs = 100)

"""### Predicting the results of the Test set"""

y_pred = ann.predict(X_test)
#just setting the precision of 2 for display
np.set_printoptions(precision=2)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

"""**Evaluating our deep learning model.**

"""

from sklearn.metrics import mean_squared_error,r2_score

#Used regression score and mean square error as two metrics for evaluation
# basically, r2_square close to 1 is a best model.
print('Regression score function reuslt : ', r2_score(y_pred,y_test))
print('mse : ', mean_squared_error(y_pred,y_test)  )